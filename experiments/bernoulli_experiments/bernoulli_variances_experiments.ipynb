{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook reproduces Bernoulli experiment results in our paper, \n",
    "\n",
    "https://arxiv.org/pdf/1810.04777.pdf\n",
    "\n",
    "Specifically, it displays the variances gains of Rao-Blackwellization shown in Figure 2. \n",
    "\n",
    "See bernoulli_optimization_experiments.ipynb for the experiments on the optimization (Figure 1). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim \n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "import bernoulli_experiments_lib as bern_lib\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../../rb_utils/')\n",
    "import optimization_lib as optim_lib\n",
    "import baselines_lib as bs_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "softmax = nn.Softmax(dim = 0)\n",
    "\n",
    "sigmoid = nn.Sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(454)\n",
    "_ = torch.manual_seed(454)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p0:  tensor([0.6000, 0.5100, 0.4800]) \n",
      "\n",
      "sum(p0^2):  tensor(0.8505)\n",
      "sum((1 - p0)^2):  tensor(0.6705) \n",
      "\n",
      "optimal loss:  tensor(0.6705)\n",
      "optimal x:  1\n"
     ]
    }
   ],
   "source": [
    "# fixed parameters\n",
    "d = 3\n",
    "# p0 = torch.rand(d)\n",
    "p0 = torch.Tensor([0.6, 0.51, 0.48])\n",
    "print('p0: ', p0, '\\n')\n",
    "\n",
    "print('sum(p0^2): ', torch.sum(p0**2))\n",
    "print('sum((1 - p0)^2): ', torch.sum((1 - p0)**2), '\\n')\n",
    "\n",
    "# the optima\n",
    "x_optimal = torch.argmin(torch.Tensor([torch.sum(p0**2), torch.sum((1 - p0)**2)]))\n",
    "\n",
    "optimal_loss = torch.min(torch.Tensor([torch.sum(p0**2), torch.sum((1 - p0)**2)]))\n",
    "\n",
    "print('optimal loss: ', optimal_loss)\n",
    "print('optimal x: ', x_optimal.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init phi0:  tensor([0.], requires_grad=True)\n",
      "init e_b:  tensor([0.5000], grad_fn=<SigmoidBackward>)\n"
     ]
    }
   ],
   "source": [
    "# random init for phi\n",
    "phi0 = torch.Tensor([0.0])\n",
    "phi0.requires_grad_(True)\n",
    "print('init phi0: ', phi0)\n",
    "print('init e_b: ', sigmoid(phi0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params = [phi0]\n",
    "optimizer = optim.SGD(params, lr = 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# True gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bern_experiment = bern_lib.BernoulliExperiments(p0, d, phi0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bern_experiment.set_var_params(deepcopy(phi0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss = bern_experiment.get_full_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0450])\n"
     ]
    }
   ],
   "source": [
    "true_grad = deepcopy(bern_experiment.var_params['phi'].grad)\n",
    "print(true_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# analytically integrate REINFORCE gradient: should recover the true gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bern_experiment.set_var_params(deepcopy(phi0))\n",
    "optimizer.zero_grad()\n",
    "ps_loss = bern_experiment.get_pm_loss(topk = 8, grad_estimator = bs_lib.reinforce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0450])\n"
     ]
    }
   ],
   "source": [
    "ps_loss.backward()\n",
    "reinforce_analytic_grad = deepcopy(bern_experiment.var_params['phi'].grad)\n",
    "print(reinforce_analytic_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert reinforce_analytic_grad == true_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Reinforce + CV gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_samples = 10000\n",
    "reinforce_grads = bern_lib.sample_bern_gradient(phi0, bern_experiment, \n",
    "                                  topk = 0, \n",
    "                                  grad_estimator = bs_lib.reinforce,\n",
    "                                  n_samples = n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true_grad:  [-0.04500002]\n",
      "mean reinforce grad:  -0.056248475\n",
      "variance:  0.443401\n",
      "scaled error:  0.01997651\n"
     ]
    }
   ],
   "source": [
    "print('true_grad: ', true_grad.numpy())\n",
    "print('mean reinforce grad: ', torch.mean(reinforce_grads).numpy())\n",
    "print('variance: ', torch.var(reinforce_grads).numpy())\n",
    "\n",
    "print('scaled error: ', (torch.std(reinforce_grads) / np.sqrt(n_samples) * 3).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'reinforce gradients')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(reinforce_grads)\n",
    "plt.axvline(x=true_grad, color = 'orange') # true gradient \n",
    "\n",
    "plt.xlabel('reinforce gradients')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rao-Blackwellize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we examine the variance of REINFORCE after using our Rao-Blackwellization procedure. We display the variance as a function of $k$, the number of categories summed.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(phi0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.], requires_grad=True)\n",
      "topk = 0\n",
      "mean reinforce grad:  -0.045077994\n",
      "variance:  0.012753774\n",
      "tensor([0.], requires_grad=True)\n",
      "topk = 1\n",
      "mean reinforce grad:  -0.045844004\n",
      "variance:  0.007630086\n",
      "tensor([0.], requires_grad=True)\n",
      "topk = 2\n",
      "mean reinforce grad:  -0.04464275\n",
      "variance:  0.0063941707\n",
      "tensor([0.], requires_grad=True)\n",
      "topk = 3\n",
      "mean reinforce grad:  -0.04470663\n",
      "variance:  0.005205873\n",
      "tensor([0.], requires_grad=True)\n",
      "topk = 4\n",
      "mean reinforce grad:  -0.044553503\n",
      "variance:  0.0036560483\n",
      "tensor([0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "n_samples = 10000\n",
    "\n",
    "mixed_grads_array = torch.zeros((2**d + 1, n_samples))\n",
    "\n",
    "for i in range(0, 2**d + 1): \n",
    "    print(phi0)\n",
    "    grads = bern_lib.sample_bern_gradient(phi0, bern_experiment, \n",
    "                                topk = i, \n",
    "                                grad_estimator = bs_lib.reinforce_w_double_sample_baseline,\n",
    "                                n_samples = n_samples)\n",
    "    \n",
    "    mixed_grads_array[i, :] = grads\n",
    "    \n",
    "    print('topk = {}'.format(i))\n",
    "    print('mean reinforce grad: ', torch.mean(grads).numpy())\n",
    "    print('variance: ', torch.var(grads).numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Lets look at the histogram of the distribution of \n",
    "# the gradient estimates\n",
    "\n",
    "for i in range(1, 2**d): \n",
    "    plt.figure()\n",
    "    plt.hist(mixed_grads_array[i-1, :])\n",
    "    plt.axvline(x=true_grad, color = 'orange') # true gradient \n",
    "    \n",
    "    plt.title('num_reinforced = {}'.format(i))\n",
    "    plt.xlabel('gradients')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fontsize = 16"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This is figure 2 (bottom) in our paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# summarize distribution all on the same plot\n",
    "# Error bars are 3 * st. error of the distribution \n",
    "\n",
    "mixed_grads_means = np.mean(mixed_grads_array.numpy(), axis = 1)\n",
    "mixed_grads_stds = np.std(mixed_grads_array.numpy(), axis = 1)\n",
    "\n",
    "plt.boxplot(mixed_grads_array, positions = range(2**d + 1))\n",
    "\n",
    "plt.plot(np.linspace(0, 2**d), true_grad.numpy() * np.ones(50), '--')\n",
    "\n",
    "plt.ylabel('Gradient estimates', fontsize = fontsize)\n",
    "plt.xlabel('Categories summed', fontsize = fontsize)\n",
    "plt.title('Gradient estimates at $\\eta$ = {}'.format(0), fontsize = fontsize)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# plt.savefig('../icml_figures/bernoulli_variances_reinforce_eta0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(np.arange(0, 2**d + 1), mixed_grads_stds, 'x-', markersize = 12)\n",
    "plt.xlabel('Number summed')\n",
    "plt.ylabel('st. dev. of gradient')\n",
    "plt.title('standard errors at $\\eta$ = {}'.format(phi0.detach().numpy()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# The variance reduction of Rao-Blackwellization is more pronounced as the variational distribution becomes more concentrated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\phi$ be equal to -4, so the b is heavily concentrated on (0, 0, 0). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "phi0 = torch.Tensor([-4.0])\n",
    "phi0.requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bern_experiment.set_var_params(deepcopy(phi0))\n",
    "class_weights = torch.exp(bern_experiment.get_log_q())\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# again assert that analyitcally summing REINFORCE returns the true gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimizer.zero_grad()\n",
    "loss = bern_experiment.get_full_loss()\n",
    "loss.backward()\n",
    "\n",
    "\n",
    "true_grad = deepcopy(bern_experiment.var_params['phi'].grad)\n",
    "print(true_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bern_experiment.set_var_params(deepcopy(phi0))\n",
    "optimizer.zero_grad()\n",
    "ps_loss = bern_experiment.get_pm_loss(topk = 8, grad_estimator = bs_lib.reinforce)\n",
    "\n",
    "ps_loss.backward()\n",
    "reinforce_analytic_grad = deepcopy(bern_experiment.var_params['phi'].grad)\n",
    "print(reinforce_analytic_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert reinforce_analytic_grad == true_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### re-examine the effect of rao-blackwellization at this new phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(phi0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_samples = 10000\n",
    "\n",
    "mixed_grads_array2 = torch.zeros((2**d + 1, n_samples))\n",
    "\n",
    "for i in range(0, 2**d + 1): \n",
    "    grads = bern_lib.sample_bern_gradient(phi0, bern_experiment, \n",
    "                                  topk = i, \n",
    "                                grad_estimator = bs_lib.reinforce_w_double_sample_baseline,\n",
    "                                  n_samples = n_samples)\n",
    "    \n",
    "    mixed_grads_array2[i, :] = grads\n",
    "    \n",
    "    print('topk = {}'.format(i))\n",
    "    print('mean reinforce grad: ', torch.mean(grads).numpy())\n",
    "    print('variance: ', torch.var(grads).numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Lets look at the histogram of the distribution of \n",
    "# the gradient estimates\n",
    "\n",
    "for i in range(1, 2**d): \n",
    "    plt.figure()\n",
    "    plt.hist(mixed_grads_array2[i-1, :])\n",
    "    plt.axvline(x=true_grad, color = 'orange') # true gradient \n",
    "    \n",
    "    plt.title('num_reinforced = {}'.format(i))\n",
    "    plt.xlabel('gradients')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is figure 2 (bottom) in our paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# summarize distribution all on the same plot\n",
    "# Error bars are 3 * st. dev of the distribution \n",
    "\n",
    "mixed_grads_means = np.mean(mixed_grads_array2.numpy(), axis = 1)\n",
    "mixed_grads_stds = np.std(mixed_grads_array2.numpy(), axis = 1)\n",
    "\n",
    "plt.boxplot(mixed_grads_array2, positions = range(0, 2**d + 1))\n",
    "\n",
    "plt.plot(np.linspace(1, 2**d + 1), true_grad.numpy() * np.ones(50), '--')\n",
    "\n",
    "plt.ylabel('Gradient estimate', fontsize = fontsize)\n",
    "plt.xlabel('Categories summed', fontsize = fontsize)\n",
    "plt.title('Gradient estimates at $\\eta$ = {}'.format(phi0.detach().numpy()[0]), \n",
    "          fontsize = fontsize)\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig('../icml_figures/bernoulli_variances_reinforce_eta_neg4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(np.arange(0, 2**d + 1), mixed_grads_stds, 'x-', markersize = 12)\n",
    "plt.xlabel('Number summed')\n",
    "plt.ylabel('st. error of gradient')\n",
    "plt.xlabel('Number summed')\n",
    "plt.ylabel('st. error of gradient')\n",
    "plt.title('standard errors at $\\eta$ = {}'.format(phi0.detach().numpy()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch_04)",
   "language": "python",
   "name": "pytorch_update"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
